This paper has presented the first generative-evaluative architecture for dexterous grasping from a single view where both the generative and evaluative models are learned. Using this architecture the top-ranked grasp success rate rises from 69.5\% (V1) to 90.49\% (V11) on a simulated test set. It also presented a real robot data set where the top ranked grasp success rate rose from 57.1\% (V1) to 87.8\% (V11).

What are the promising lines of enquiry to further improve dexterous grasping of unfamiliar objects? We see three major issues. First, we have assumed no notion of object completion. Humans succeed in grasping in part because we have strong priors on object shape that help complete the missing information. This would enable the deployment of a generative model that exploits a more complete object shape model \cite{kopicki2015ijrr}. Second, our approach is open-loop during execution. For pinch-grasping, deep nets have been shown to learn useful visual servoing policies \cite{morrison18}. However, significant gains will also come from post-grasp force-control strategies, which are largely absent from the literature on grasp learning.  Third, the architectural scheme presented here is essentially that of an actor-critic architecture. This suggests incremental refinement of both the generative model and the evaluative model, perhaps using techniques from reward based learning. We have already shown elsewhere that the GM may be further improved by training from autonomously generated data \cite{kopicki2019ijrr}. Data intensive generative models also hold promise \cite{veres2017modeling} and it may be possible to seed them by training with example grasps drawn from a data-efficient model such as that presented here.\\