This paper has presented an architecture for dexterous grasping from a single view where both the generative and evaluative models are learned. It has introduced a simulated dataset of 2.4 million dexterous grasps based on two generative methods. Multiple evaluative models were trained and tested. Using the architecture, the top-ranked grasp success rate  rises from 69.5\% (V1) to 90.49\% (V11) on a simulated test set. It also presented a real robot data set where the top ranked grasp success rate rose from 57.1\% (V1) to 87.8\% (V11).

There are several limitations of the method, including relative to others in the literature. First, we note that while our data set is challenging our success rates on real objects are competitive with, but not significantly better than other recent methods that employ deep learning for dexterous grasping, although we note that the variety of objects we grasp is somewhat greater. We do not, however, believe that training with ever greater amunts of data is the only forward. There are other approaches. For example, here, we have assumed no notion of object completion. Humans succeed in grasping in part because we have strong priors on object shape that help complete the missing information. This would enable the deployment of a generative model that exploits a more complete object shape model \cite{kopicki2015ijrr}. Second, our approach, like that of most others to dexterous grasping, is open-loop during execution. For pinch-grasping, deep nets have been shown to learn useful visual servoing policies \cite{morrison18}. However, significant gains will also come from post-grasp tactile or force-control strategies such as \cite{Torres2018}.

What other possibilities for future work are there? First, the architectural scheme presented here is essentially that of an actor-critic architecture. This suggests continual improvement of both the generative model and the evaluative model, perhaps using techniques from reward based learning. We have already shown elsewhere that the GM may be further improved by training from autonomously generated data \cite{kopicki2019ijrr}. Other work reviewed here \cite{mandikal2021dexterous,Osa2018} has already started down the road of reinforcement learning for dexterous grasping. Data intensive generative models also hold promise \cite{veres2017modeling,Shao2020,Shang2020,Zhao2020} and it may be possible to seed their training with example grasps drawn from a data-efficient model such as that presented here.\\