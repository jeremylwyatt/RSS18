This paper has presented a generative-evaluative architecture for dexterous grasping from a single view where both the generative and evaluative models are learned. \textcolor{red}{It has introduced a grasp simulator and a simulated dataset of 2.4 million dexterous grasps based on two generative methods. Multiple versions of evaluative models were trained and evaluated.} Using the generative-evaluative architecture, the top-ranked grasp success rate rises from 69.5\% (V1) to 90.49\% (V11) on a simulated test set. It also presented a real robot data set where the top ranked grasp success rate rose from 57.1\% (V1) to 87.8\% (V11).

\textcolor{red}{There are several limitations of the method relative to others in the literature. We see these limitations.} First, we have assumed no notion of object completion. Humans succeed in grasping in part because we have strong priors on object shape that help complete the missing information. This would enable the deployment of a generative model that exploits a more complete object shape model \cite{kopicki2015ijrr}. Second, our approach is open-loop during execution. For pinch-grasping, deep nets have been shown to learn useful visual servoing policies \cite{morrison18}. However, significant gains will also come from post-grasp tactile or force-control strategies such as \cite{Torres2018}. 

\textcolor{red}{What possibilities for future work are there? First,} the architectural scheme presented here is essentially that of an actor-critic architecture. This suggests incremental refinement of both the generative model and the evaluative model, perhaps using techniques from reward based learning. We have already shown elsewhere that the GM may be further improved by training from autonomously generated data \cite{kopicki2019ijrr}. Other work reviewed here \cite{mandikal2021dexterous,Osa2018} has already started down the road of reinforcement learning for dexterous grasping. Data intensive generative models also hold promise \cite{veres2017modeling} and it may be possible to seed them by training with example grasps drawn from a data-efficient model such as that presented here.\\