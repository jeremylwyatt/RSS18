%% This section describes the improved generative method. Only the differences between the original method (given in the previous section) and the new methhod are explained here. 

In this paper we also utilised a more advanced generative model, which we refer to as GM2. This model has four features which are different from the base model GM1. These are not a contribution of the paper and are described fully in \cite{kopicki2019}. For completeness we briefly describe the two differences between GM2 and GM1 at the learning state. There is also a third innovation, which is employed in the inference stage and which we describe later.

\subsection{Object View Model}\label{sec:representations.object}
The first difference is that the learning of grasp models is done per view, rather than per grasp. This means that for a training grasp made on an object viewed from seven viewpoints that there will be seven grasp models learned. This enables grasps to generalise better when the testing object to be grasped is thick and is only seen from a single view. The view based models allow a greater role to be played by the hand shape model and this enables generated grasps to have fingers that 'float' behind a back surface that cannot be seen by the robot.

\subsection{Clustering Contact Models}\label{sec:learning.clustering}

The second innovation is the ability to merge grasp models learned from different grasps. Using the memory based scheme of GM1, the number of contact models $N_{\mathcal{M}}$ equals the product of the number of training grasps by the number of views. This has two undesirable properties. First, it means that generation of grasps for test objects rises linearly in the number of training grasps. Second, it limits the generalisation power of the contact models. We can overcome both these problems by clustering the contact models from each training grasp. To do this we need a measure of the similarity between any pair of contact models. We devised a simple asymmetric divergence, which is quick to compute. We then build on top of it a symmetric distance. Having obtained this distance measure we can employ our clustering method of choice, which in our case was affinity propagation \cite{frey2007clustering}. After clustering, we compute a cluster prototype as described in \cite{kopicki2019}.
