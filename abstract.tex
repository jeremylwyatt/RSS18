Dexterous grasping of a novel object given a single view is an open problem. This paper makes several contributions to the solution of this problem. First, we present a data set of more than two million simulated dexterous grasps for 294 objects drawn from 20 categories. Second, we present a basic architecture for generation and evaluation of dexterous grasps that is trained on a portion of this data set. Third, we train and evaluate eight different architectural variants on this data set. Finally, we present a real robot implementation and evaluate the most promising methods on data set of real object-pose pairs. We show that our best architectural variant achieves a success rate of 87.8\% on novel objects seen from a single view. This improves on the state of the art. 

%Deep neural networks (DNNs) have been used to learn evaluative models (EMs). Given a grasp and an image, an EM indicates the probability of grasp success. Finding a grasp is then an optimisation problem on this evaluation function, searching over the grasp configuration space. This works well for pinch grasps, but it is an open question whether this approach generalises well to dexterous grasps, where the configuration space is of high dimension. An alternative is to learn a generative model (GM), which maps from images to grasps, subsequently refined by search. Factored GMs scale to high-dimensional configuration spaces, allow data-efficient learning, and generate a wide variety of grasp types, fully exploiting the possibilities of dexterous hands. But they give no guarantee as to the probability of grasp success. This paper shows the benefits of a hybrid architecture. It presents and compares multiple versions of three architectures for dexterous grasping: i) pure GM; ii) pure EM and iii) hybrid GM-EM. Extensive empirical studies were executed in both simulation and on a real robot. These show that hybrid GM-EM outperforms pure GM,  which in turn outperforms pure EM. The best performing GM-EM model achieves 87.7\% on a real robot dexterously grasping 49 novel objects in challenging poses.


%A generative-evaluative learning architecture (GEA) is presented. The generative model (GM) is acquired by data efficient learning from demonstration (LfD), and the evaluative model (EM) is trained in simulation, using grasps proposed by the generative model. When a novel object is presented the generative model proposes grasps, which are ranked by the evaluative model. Experiments show that this GEA architecture improves on a pure generative model (GM). On a challenging set of 49 real objects, GEA has a grasp success rate of 77.6\% relative to a pure GM (57.1\%). It is also shown that grasp optimisation using the EM fails to improve on grasps suggested by the GEA, worsening grasp success rate in simulation by 4.8\% against the baseline. These results provide support for generative-evaluative learning  for dexterous grasping.