%Introductory sentences here
In this section, we describe how we generated a realistic simulated data set for dexterous grasping. This captures variations in both observable (e.g. object pose) and unobservable (e.g. surface friction) parameters.

To generate the training set, a simulated depth image of a scene containing a single unfamiliar object is generated. Using either of the generative models GM1 or GM2, grasps are generated and executed in simulation. The success or failure of each simulated grasp is recorded. An important aspect of a simulated grasping data set is that it must capture the natural uncertainty in unobservable variables, such as mass and friction. Since many of these parameters are unobservable we are thus creating a data set such that the grasp policy must work across a range of variations. This is thus a form of {\em domain randomisation}. A similar technique has been employed by \cite{mahler2017dex}, but we extend it from a single grasp quality metric to full rigid body simulation.

\subsection{Features and Constraints of the Virtual Environment}
\label{subsection:environment}

\begin{figure}[t]
\begin{center}
  \includegraphics[width=0.7\columnwidth]{images/allObjects-small.pdf}
  \end{center}
  \caption{A sample of the 294 objects from all 20 object classes.
  \label{fig:allObjects}}
\end{figure}

The collected 3D model dataset contains 294 objects from 20 classes, namely, bottles, bowls, cans, boxes, cups, mugs, pans, salt and pepper shakers, plates, forks, spoons, spatulas, knives, teapots, teacups, tennis balls, dustpans, scissors, funnels and jugs (Figure \ref{fig:allObjects}). The number of objects in each class varies from 1 (dustpan) to 25 (bottles). Long/thin objects such as kitchen utensils are placed vertically in a short, heavy cup. This reflects the real-world scenario, as attempting to grasp a spatula lying on a table would be dangerous for the robotic hand. In total, 250 objects from all classes were allocated for training and validation, while the remaining 44 objects from 19 classes belong to the test set.

To achieve domain randomisation, prior distributions for mass and size were estimated from real-world data. The properties of simulated objects are sampled from these priors. The friction coefficient of each object is uniformly sampled from a range of $[0.5, 1]$ in MuJoCo default units, intended to simulate surfaces from low-friction (glass) to high-friction (rubber). This variation is critical to ensuring that the evaluative model will predict the robustness of a grasp in unobservable variations.

%During the scene creation, the object is placed on the virtual table at a pseudo-random pose. Most objects are placed in a canonical upright pose, and only randomly rotated around the gravity axis (akin to a turntable). The objects belonging to the mug and cup classes have fully random 3D rotations.

We employ MuJoCo \cite{MuJoCo} as the rigid-body simulator. Since MuJoCo requires that objects comprise of convex parts, all 294 objects were decomposed into convex parts using V-HACD algorithm \cite{V-HACD}. The number of sub-parts varies from 2 to 120. For depth image simulation the Carmine 1.09 depth sensor installed on the robot is simulated with a modified version of the Blensor Kinect sensor simulator \cite{KinectSimulator}. A 3D mesh-model of the DLR-II hand has been used in the simulator. There are no kinematic constraints on how the hand may grasp an object, other than collisions with the table. To ensure realism, we use impedance control for the hand.
%The reasons for the most critical of these decisions are now given in slightly more detail. First, in order to create a realistic simulation environment, we chose the MuJoCo \cite{MuJoCo} physics simulator over other simulators (OpenSim, BulletPhysics, ODE, NVIDIA PhysX) for two reasons: 
%\begin{itemize}
%\item MuJoCo uses generalized coordinates and optimization-based contact dynamics, resulting in fewer numerical instabilities,
%\item MuJoCo is optimized for the quality of physics as well as its speed, hence improving the quality of the physics simulation.
%\end{itemize}



%Table \ref{fig:graspperf} shows the success rates of the generated grasps in each class, when attempted with the grasps ranked by the Generative Model (GM1). The sampled grasps perform well on a number of classes including Dustpans, Scissors, Spoons, and Mugs. Some objects can only be grasped in certain ways, i.e. not all 10 training grasps are applicable to all objects.

%\begin{table}[]
%\centering
%\small
%\caption{The average and \textbf{top} grasp success rates (\%) of GM1 on simulated data.}
%\label{fig:graspperf}
%\begin{tabular}{|l|l|l|l|l|l|l|}
%\hline
%Bottle & Bowl     & Box     & Can     & Cup    & Fork    & Pan     \\ \hline
%35 - \textbf{47} & 26 - \textbf{61}   & 16 - \textbf{30}  & 41 - \textbf{92} & 44 - \textbf{59} & 59 - \textbf{68}   & 37 - \textbf{57} \\ \hline
%Plate  & Scissors & Shaker  & Spatula & Spoon  & Teacup  & Teapot  \\ \hline
%50 - \textbf{95}  & 62 - \textbf{69}   & 47 - \textbf{53} & 57 - \textbf{65}   & 63 - \textbf{82}  & 48 - \textbf{91} & 26 - \textbf{23} \\ \hline
%Jug    & Knife    & Mug     & Funnel  & Ball   & Dustpan &         \\ \hline
%24 - \textbf{43} & 58 - \textbf{65}   & 40 - \textbf{80} & 52 - \textbf{65}   & 28 - \textbf{82}  & 60 - \textbf{78} & 45 - \textbf{63} \\ \hline
%\end{tabular}
%\end{table}

%\begin{table}[]
%\centering
%\caption{Mass ranges for each object class (grams).}
%\label{fig:weights}
%\resizebox{\linewidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|}
%\hline
%Bottle & Bowl     & Box     & Can     & Cup    & Fork    & Pan     \\ \hline
%35.5 - \textbf{47.7}\% & 26.4 - \textbf{61.2}\%   & 16.5 - \textbf{30.1}\%  & 41.4 - \textbf{92.6}\% & 44.7 - \textbf{59.9}\% & 59.6 - \textbf{68.1}\%   & 37.9 - %\textbf{57.3}\% \\ \hline
%Plate  & Scissors & Shaker  & Spatula & Spoon  & Teacup  & Teapot  \\ \hline
%50.2 - \textbf{95.5}\%  & 62.7 - \textbf{69.9}\%   & 47.3 - \textbf{53.3}\% & 57.4 - \textbf{65.7}\%   & 63.4 - \textbf{82.4}\%  & 48.2 - \textbf{91.2}\% & 26.9 - %\textbf{23.9}\% \\ \hline
%Jug    & Knife    & Mug     & Funnel  & Ball   & Dustpan &         \\ \hline
%24.9 - \textbf{43.9}\% & 58.3 - \textbf{65.0}\%   & 40.7 - \textbf{80.9}\% & 52.3 - \textbf{65.9}\%   & 28.0 - \textbf{82.8}\%  & 60.1 - \textbf{78.8}\% & 45.8 - %\textbf{63.2}\%        \\ \hline
%\end{tabular}}
%\end{table}

\subsection{Data Collection Methodology}
\label{subsection:dataCollection}

The data set is divided into units called \textit{scenes}, where each scene comprises a single object placed on a table. This object has a specific set of physical parameters, as described below. Many views and grasps are attempted per scene. Below, we specify the time flow of data collection:

\begin{enumerate}
\item A novel instance of an object from the dataset is generated and placed on a virtual table in a random pose. Variations are applied to object scale, mass, and friction coefficients.
\item A simulated camera takes a depth image $I_s$ of the scene, converted to a point cloud $P_s$. The viewpoint ${elevation}_s$ of the view point is from 30-57 degrees. The ${azimuth}_s$ is sampled from $[0, 2\pi]$. 
\item All points in the point cloud $P_s$ are shifted by a three-dimensional vector sampled from a Gaussian distribution with parameters $\mu=0$ and $\sigma = 0.004$ (unit: meter), to simulate camera calibration errors.
\item Given $P_s$, the chosen generative model (GM1 or GM2) proposes the candidate grasps. For GM1 and GM2, we choose up to 10 and 50 top grasps per each one of the 10 training grasps, respectively.
\item The grasps are applied to the object in simulation. Before the execution of each grasp, we run a collision check with the virtual table (without the object). The grasps that fail this test are marked as \textit{collided}.
\item 19 further simulated depth images are taken from other viewpoints around the object, as explained in step 2. Images with fewer than 250 depth points are discarded. We then sample with replacement from the remaining images and associate each sampled image and viewpoint with a grasp created in step 3.
\item The grasp outcome, trajectory and depth image are stored for each trial. The grasp parameters are converted to the camera frame for the associated view.
\end{enumerate}

%Each candidate grasp $h_i = \{w_0, ..., w_{n}\}$ consists of a series of 10 waypoints along : $w_0$, ..., $w_{n}$. A waypoint $w_k$ is a 27-element vector that specifies full configuration of the hand in joint space: 3 dimensions for 3D coordinates and 4 dimensions for the orientation of the wrist, and 20 parameters specifying each finger joint's activation. 
%After a grasp $h_i$ is generated in world coordinates, the waypoints that belong to the grasp are converted to the camera's frame of reference. 
%The goal of our network architecture is to learn which grasps are more likely to succeed given a point cloud, where both input channels are represented in terms of the camera frame of reference. %This point differentiates us from the work of Levine et al. \cite{Levine1}, where camera coordinates are not used. It should be noted that the possible camera locations in our simulated data covers a larger space, with full circular movement $[0, 2\pi]$ on azimuth and $[30-57]$ range in elevation. Our scenes do not have any distinguishing landmarks such as a bin or robot base, which may aid the network in locating the camera in the scene. 

In each scene $S_i$, a number of depth images are taken $\{I_{ik}\}_{k=0}^{20}$, in the manner explained above. The first image $I_{i0}$ is used to generate grasps using GM1 \cite{kopicki2015ijrr} or GM2 \cite{kopicki2019ijrr}. We typically perform 60-300 grasps per scene. Attaching different views to each grasp instead of the seed image $I_{i0}$ ensures there is more variation in terms of viewpoints, resulting in a richer dataset. Once a grasp is performed in simulation, it is considered a success if an object is lifted one metre above the table, and held there for two seconds. If the object slips from the hand during lifting or holding, the grasp is a failure. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\columnwidth]{images/frictionweight}
\end{center}
%\includegraphics[width=0.24\textwidth]{images/Pan4_2_HFLW}
%\includegraphics[width=0.24\textwidth]{images/Pan4_2_LFLW}
%\includegraphics[width=0.24\textwidth]{images/Pan4_2_HFHW}
%\includegraphics[width=0.24\textwidth]{images/Pan4_2_LFHW}\\
%%\includegraphics[width=0.96\textwidth]{images/key-to-eval-training}\\
%\includegraphics[width=0.24\textwidth]{images/Pan4_HFLW}
%\includegraphics[width=0.24\textwidth]{images/Pan4_LFLW}
%\includegraphics[width=0.24\textwidth]{images/Pan4_HFHW}
%\includegraphics[width=0.24\textwidth]{images/Pan4_LFHW}
\caption{Creating a data set for robust evaluation. (Top row) The same pinch grasp, executed on the same object, with varying friction and mass parameters. (Bottom row) A more robust power grasp, executed on the same object, with the same variation in friction and mass. \label{fig:evaluative-training}}
\end{figure}

Using this method, we generated a data set (DS1) of 1.28m simulated grasps using GM1 as the generative model and a data set of 1.136m additional grasps (DS2) using GM2. Each grasp in DS1-Te and DS2 can be replayed in MuJoCo and the sets are split into train, validation and test sets (Table~\ref{tab:data}). The ratio of successful grasps in the dataset is less than 50\% for GM1, and is more than 50\% for GM2. DS1 and DS2 only contain scenes that have at least one successful grasp to have a balanced data set. During training, the datasets were balanced by under-sampling the failure cases in DS1-Tr and over-sampling the failure cases for DS2-Tr. No balancing was performed for the validation and test sets.
\begin{table*}[t]
\centering
\caption{Statistics of the simulated data sets.}
\label{tab:data}
\resizebox{\columnwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|} \hline
Data set & Generative &  Subset & \# Scenes & Top-grasp & Top-grasp & Top grasp & Total & Total  & Total  & Total \\ 
              & Model         &              &                   &  \# succs  & \# fails       & \% succs  & grasps   & \# succs      & \# fails  & \% succs  \\ \hline
 DS1-Tr & GM1 & Train & 17714 & 10100 & 7614 & 57.0\% & 1,058,430 & 479,941 & 578,489 & 45.3\% \\ \hline
 DS1-V  & GM1 & Validate & 2309 & 1290 & 1019 & 55.9\% & 122,944 & 61,256 & 61,688 & 49,8\% \\ \hline
 DS1-Te & GM1& Test & 1539 & 1070 & 469 & 69.5\% & 99,521 & 48,084 & 51,437 & 48.3\% \\ \hline
 DS2-Tr  & GM2 & Train & 5377 & 3771 & 1606 & 70.1\% & 943,481 & 533,282 & 410,199 & 56.5\% \\ \hline
 DS2-V   & GM2 & Validate & 544 & 378 & 166 & 69.4\% & 68,586 & 39,559 & 29,027 & 57.7\% \\ \hline
 DS2-Te  & GM2 & Test & 988 & 781 & 207 & 79.0\% & 124,137 & 73,836 & 50,301 & 59.5\% \\ \hline
\end{tabular}}
\end{table*}

