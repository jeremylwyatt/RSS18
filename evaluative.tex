The grasping system proposed, shown in Figure \ref{fig:systemArchitecture}, consists of a learned generative model and an evaluative model. The generative model is a method that generates a number of candidate grasps given a point cloud, as explained in the previous section. An evaluative model is paired with a generative model in order to estimate a probability of success for each candidate grasp. EM1 and EM3 are VGG-16 based visual models, while EM2 is a modified ResNet-50 architecture. All evaluative models process the visual data and hand trajectory parameters in two separate pathways, and combine them to feed into a third processing block to produce the final success probability. In addition, we will present techniques for grasp optimisation using the EM as the objective function, using both Gradient Descent (GD) and Simulated Annealing (SA). Finally, we may train each model with either the data set of simulated grasps generated by GM1, by GM2, or by both combined. Table \ref{table:GEBreakdown} shows a the full list of 17 variants we test.

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Variant & GM/  & EM & Opt'  & Training Set \\ 
 & Testset & & Meth' & \\ \hline
V1 & GM1    & - & - & 10 grasps  \\ \hline
V2 & GM2    & - & - & 10 grasps  \\ \hline
V3 & GM1/DS1-Te & EM1 & - & DS1-Tr \\ \hline
V4 & GM1/DS1-Te & EM2 & - & DS1-Tr \\ \hline
V5 & GM1/DS1-Te & EM3 & - & DS1-Tr  \\ \hline
V6 & GM1/DS1-Te & EM1 & - & DS1-Tr + DS2-Tr \\ \hline
V7 & GM1/DS1-Te & EM2 & - & DS1-Tr + DS2-Tr \\ \hline
V8 & GM1/DS1-Te & EM3 & - & DS1-Tr + DS2-Tr \\ \hline
V9 & GM2/DS2-Te & EM1 & - & DS1-Tr + DS2-Tr \\ \hline
V10 & GM2/DS2-Te & EM2 & - & DS1-Tr + DS2-Tr \\ \hline
V11 & GM2/DS2-Te & EM3 & - & DS1-Tr + DS2-Tr \\ \hline
V12 & GM1/DS1-Te & EM1 & GD & DS1-Tr + DS2-Tr \\ \hline
V13 & GM1/DS1-Te & EM2 & GD & DS1-Tr + DS2-Tr \\ \hline
V14 & GM1/DS1-Te & EM3 & GD & DS1-Tr + DS2-Tr \\ \hline
V15 & GM1/DS1-Te & EM1 & SA & DS1-Tr + DS2-Tr \\ \hline
V16 & GM1/DS1-Te & EM2 & SA & DS1-Tr + DS2-Tr \\ \hline
V17 & GM1/DS1-Te & EM3 & SA & DS1-Tr + DS2-Tr \\ \hline
\end{tabular}
\caption{The evaluated combinations of architecture, generative model/test set, training set, and optimisation method (Gradient Descent (GD) or Stochastic Simulated Annealing (SA).}
\label{table:GEBreakdown}
\end{table}

In this section, the three proposed evaluative model (EM) architectures are explained. The grasp generator models, GM1 and GM2, given in the previous section, require very little training data to train, here being trained from 10 example grasps. %GM1 can generate 500 candidate grasps, ranked according to their estimated likelihoods, within 10 seconds on a 2x Intel Xeon E5-2650 v2 Eight Core 2.6GHz. GM2 takes a 50 seconds to create 250 grasps in the same setting. 
These generative models do not, however, estimate a probability of success for the generated grasps. An evaluative model, which is a Deep Neural Network (DNN), is used specifically for this purpose. DNNs have shown good performance in learning to evaluate grasps using two-finger grippers \cite{levine16,lenz2015deep}. They have also been applied to generating pre-grasps, so as to perform power grasps with dexterous hands \cite{varley2015generating,lu2017planning}.

%The generative approach ignores the global information about the object, such as overall shape and the object category. The success of an executed grasp, however, depends on many contextual factors such as full object shape, mass, mass distribution and surface friction. An evaluative network can indirectly learn to predict grasp success from image data. 
%The data provided to the evaluative network is collected from randomly generated scenes, therefore each scene has a different random combination of the parameters. The primary purpose of the network is to learn robust grasps across different conditions, and this is a complex task. The first challenge is that the kinematic model of the hand is unknown to the evaluative network. It only has access to the parameters that \textit{configure} the hand: the wrist and joint positions. Second, the system is weakly supervised with the grasp result (success or failure), and no further labels are provided.

We tested three evaluative models. The first is based on the VGG-16 network \cite{Simonyan14c}, named Evaluative Model 1 (EM1), and shown in Figure \ref{fig:networkArchitecture2} (a). A version based on the ResNet-50 network, termed EM2, is shown in Figure \ref{fig:networkArchitecture2} (b). Finally, EM3 (Figure \ref{fig:networkArchitecture2} (c) is also based on VGG-16. Regardless of the type, a grasp evaluation network has the functional form $f(I_t, h_t)$, where $I_t$ is a colourised depth image of the object, and $h_t$ contains a series of wrist poses and joint configurations for the hand, converted to the camera's frame of reference. The network's output layer calculates a probability of success for the image-grasp pair $I_t$, $h_t$. The model initially processes the grasp parameters and visual information in separate channels, and combines them to feed into a feedforward pipeline that produces the output. 

%\begin{figure}[h]
%  \includegraphics[width=0.9\linewidth]{images/colourDepth.pdf}
%  \caption[Colourised depth images.]{Colourised depth images. From left to right, the objects are: coke bottle, chocolate box, hand cream, and bowl.}
%\label{fig:colorisedDepth}
%\end{figure}

The depth image is colourised before it is passed as input to the evaluative network. The colourisation process converts the single-channel depth data to a 3-channel RGB image. We first crop the middle $460 \times 460$ section of the $640 \times 480$ depth image, and down-sample it to $224 \times 224$. Then, two more channels of the same dimension are added corresponding to the mean and Gaussian curvatures. Figure~\ref{fig:colorisedDepth} contains four examples of colourised depth images. This procedure both provides a meaningful set of depth features to the network, and makes the input compatible with VGG-16 and ResNet, which require images of size $224 \times 224 \times 3$.

The grasp parameter data $h_t$ consists of 10 trajectory waypoints represented by $27 \times 10 = 270$ floating point numbers, and 10 extra numbers reserved for the grasp type. Each of the 10 training grasps is treated as a different class, and $h_t$ uses the 1-of-N encoding system. Based on the grasp type ([1-10]), the corresponding entry is set to 1, while the rest remain 0. The grasp parameters are converted to the coordinate system of the camera which was used to obtain the corresponding depth image. The transformed parameters are processed using a fully-connected (FC) layer consisting of 1024 nodes, and the output is \textit{element-wise added} to the visual features. The joint visual features and grasp parameter data are processed together in higher layers.

All FC layers have RELU activation functions, except for the output layer, which has softmax activations. The output layer has two nodes, corresponding to the success and failure probabilities of the grasp. A cross-entropy loss is used to train the neural network, as given in \eq\ref{equation:crossentropy}.

\begin{equation}
H_{y'}(y) := - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log (1-y_i)})
\label{equation:crossentropy}
\end{equation}
where $y_i'$ is the class label of the grasp, which is either 1 (success) or 0 (failure), and $y_i = f(I_i, h_i)$ is is the predicted label of the grasp pair ($I_i$, $h_i)$.

The proposed evaluative models EM1-3 share the common features explained above. The individual models are now introduced below. Only their unique properties are highlighted.

\begin{figure*}[t]
\centering
% \begin{center}
\subfloat[Evaluative Model 1]{%
  \includegraphics[width=0.95\textwidth]{images/networkArchitecture.pdf}
}
% \end{center}

% \begin{center}
\subfloat[Evaluative Model 2]{%
    \includegraphics[width=0.85\textwidth]{images/ResNet.pdf}
}
% \end{center}

% \begin{center}
\subfloat[Evaluative Model 3]{%
    \includegraphics[width=0.75\textwidth]{images/ChaoNet.png}
}
% \end{center}

\caption{The three evaluative network architectures presented in this paper. Similar to \cite{Levine1}, the two channels of information (visual data and grasp parameters) are processed in parallel and combined to reach the final decision in each model. RELU activations are used throughout the models, except for the final softmax layers. A final softmax layer has grasp success and and failure nodes, and learns to predict the success probability of a grasp. (a) VGG-16 based model in our previous paper \cite{icra}. The first 13 convolutional layers of VGG-16 are frozen. (b) ResNet50-based \cite{HeZRS15} proposed network. The first four blocks are used for feature extraction, and the rest of the network is used to learn joint features. (c) Third proposed model based on the VGG-16 architecture. The information pathways are joined via concatenation, not addition.
\label{fig:networkArchitecture2}}
\end{figure*}

\subsection{Evaluative Model 1 (EM1)}

Figure~\ref{fig:networkArchitecture2} (a) shows the architecture of the first proposed evaluative network. The colourised depth image is processed with the VGG-16 network \cite{Simonyan14c} to obtain the high-level image features. The VGG-16 network is initialised with the weights obtained from ImageNet training. Only the last three convolutional layers are trained, and the first 13 layers remain frozen. This decision was made in order to speed up training.

The grasp parameters and image features pass through fully-connected layers with 1024 hidden nodes (FC-1024) layers in order to obtain two feature vectors of length 1024. The features are combined using the element-wise addition operation, and are further processed using 4 FC-1024 layers. Similarly with \cite{Levine1}, the features are combined using addition and not concatenation. In this thesis, this decision was made based on the observation that addition yielded a marginally better performance in the experiments (not included in this thesis). Furthermore, concatenation and addition can be considered as interchangeable operations when combining different information pathways in deep networks \cite{dumoulin2018feature-wise}. The final FC-1024 layers form the associations between the visual features and hand parameters, and contain most of the parameters in the network. 

\subsection{Evaluative Model 2 (EM2)}

% \begin{figure}[!ht]
%   \includegraphics[width=\textwidth]{images/ResNet.pdf}
%   \caption[The ResNet-based evaluative deep neural network architecture.]{The ResNet-based evaluative deep neural network architecture (EM2). Spatial tiling is used to repeat the grasp parameters before they join the image processing pathway. This network requires fewer FC layers due to the earlier marriage of information channels.}
% \label{fig:ResNet}
% \end{figure}

The second evaluative model, termed EM2 (Figure~\ref{fig:networkArchitecture2} (b)), uses the ResNet-50 network in order to obtain the high-level image features. The ResNet-50 architecture is initialised with the weights obtained from ImageNet training. One exception is the last convolution block conv\_5x, which is initialised randomly. In the EM2 architecture, ResNet-50 network is broken down into two parts: the first 4 convolutional blocks are used to extract the visual features. The final convolutional block, which has 9 convolutional layers, combines the image features and grasp parameters. Similarly with EM1, element-wise addition joins the two channels of information. Spatial tiling is used to convert the processed grasp parameters, a vector of size $1024$, to a matrix of size $14 \times 14 \times 1024$. Because the last convolutional block conv\_5x processes combined information, this network is designed with only 2 FC layers with 64 hidden nodes each. The output layer is the same as EM1. 

\subsection{Evaluative Model 3 (EM3)}

This model, similar to EM1 (Figure~\ref{fig:networkArchitecture2} (c)), uses VGG-16 as the visual model. All 16 layers of VGG-16 are unfrozen. The hand trajectory parameters pass through a feature extraction network before being concatenated with the visual features. Two fully connected layers of higher capacity than EM1-2, contain 4096 hidden nodes each and join the two pathways. A final 2-node FC layer with softmax activation is added to obtain the final decision.

TODO: More info will be added here after getting various details from Chao, including custom feature extraction layer and output dimensionality for VGG-16.

All models were trained on simulated data and tested on the real robot, as well as in simulation. The next section focuses on the collection of the training data.

\subsection{EM Training Methodology}
Variants V3-V5 were trained using DS1-Tr. Variants V6-V17 were trained using the combined data set from DS1-Tr and DS2-Tr. The ADAM optimiser was employed with learning rate 0.001, a dropout rate of 0.3, and early stopping.