The grasping system proposed, shown in Figure \ref{fig:systemArchitecture}, consists of a learned generative model and an evaluative model. The generative model is a method that generates a number of candidate grasps given a point cloud, as explained in the previous section. An evaluative model is paired with a generative model in order to estimate a probability of success for each candidate grasp. EM1 and EM3 are VGG-16 based visual models, while EM2 is a modified ResNet-50 architecture. All evaluative models process the visual data and hand trajectory parameters in two separate pathways, and combine them to feed into a third processing block to produce the final success probability. The generative models GM1 and GM2, and the evaluative models EM1-3 are interchangeable, creating 6 generative-evaluative model pairs. Table \ref{table:GEBreakdown} shows a the full list of generative-evaluative algorithms and their respective building blocks.

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Model Name & GE1 & GE2 & GE3 & GE4 & GE5 & GE6 \\ \hline
GM         & GM1 & GM1 & GM1 & GM2 & GM2 & GM2 \\ \hline
EM         & EM1 & EM2 & EM3 & EM1 & EM2 & EM3 \\ \hline
\end{tabular}
\caption{Building blocks of the proposed generative-evaluative methods.}
\label{table:GEBreakdown}
\end{table}

In this section, the three proposed evaluative model architectures are explained. The grasp generator models, GM1 and GM2, given in the previous section, require very little training data to train. GM1 can generate 500 candidate grasps, ranked according to their estimated likelihoods, within 10 seconds on a 2x Intel Xeon E5-2650 v2 Eight Core 2.6GHz. GM2 takes a 50 seconds to create 250 grasps in the same setting. The generative models do not estimate a probability of success for the generated grasps. An evaluative model, which is a Deep Neural Network (DNN), is used specifically for this purpose. DNNs have shown good performances in learning to evaluate grasps using grippers \cite{levine16,lenz2015deep} and dexterous hands \cite{varley2015generating,lu2017planning}.

The generative approach ignores the global information about the object, such as the overall shape and the object class, to a large extent. It takes the robot hand model and the local object shape features into account. Moreover, it only has access to the partial object shape, due to the noise in the image acquisition process and self-occlusions. The success of an executed grasp, however, depends on other factors such as the full object shape, mass, mass distribution, surface friction and deformability, among others. An evaluative network indirectly learns the effect of these parameters on the grasp outcome. The data provided to the evaluative network is collected from randomly generated scenes, therefore each scene has a different random combination of the parameters. The primary purpose of the network is to learn robust grasps across different conditions, and this is a complex task. The first challenge is that the kinematic model of the hand is unknown to the evaluative network. It only has access to the parameters that \textit{configure} the hand: the wrist and joint positions. Second, the system is weakly supervised with the grasp result (success or failure), and no further labels are provided.

The proposed evaluative models have similar characteristics. The first evaluative DNN based on the VGG-16 network \cite{Simonyan14c}, named Evaluative Model 1 (EM1), is given in Figure \ref{fig:networkArchitecture2}.a. A slightly different version based on the ResNet-50 network, termed Evaluative Model 2 (EM2), is shown in Figure \ref{fig:networkArchitecture2}.b. Finally, our second DNN which is based on the VGG-16 network can be seen in Figure \ref{fig:networkArchitecture2}.c. Regardless of the type, a grasp evaluation network has the functional form $f(I_t, h_t)$, where $I_t$ is a colourised depth image of the object, and $h_t$ contains a series of wrist poses and joint configurations for the hand, converted to the camera's frame of reference. The network's output layer calculates a probability of success for the image-grasp pair $I_t$, $h_t$. The model initially processes the grasp parameters and visual information in separate channels, and combines them to learn the final output. 

\begin{figure}[h]
  \includegraphics[width=0.9\linewidth]{images/colourDepth.pdf}
  \caption[Colourised depth images.]{Colourised depth images. From left to right, the objects are: coke bottle, chocolate box, hand cream, and bowl.}
\label{fig:colorisedDepth}
\end{figure}

The depth image is colourised before it is passed as input to the evaluative network. The colorisation process converts the single-channel depth data to a 3-channel RGB image. We first crop the middle $460 \times 460$ section of the $640 \times 480$ depth image, and downsample it to $224 \times 224$. Then, two more channels of information are added: mean and Gaussian curvatures, obtained from the object's surfaces in the image. Figure \ref{fig:colorisedDepth} contains four examples of colourised depth images. This procedure both provides a meaningful set of features to the network, and makes the input directly compatible with VGG-16 and ResNet, which require images of size $224 \times 224 \times 3$.

The grasp parameter data $h_t$ consists of 10 trajectory waypoints represented by $27 \times 10 = 270$ floating point numbers, and 10 extra numbers reserved for the grasp type. Each of the 10 training grasps is treated as a different class, and $h_t$ uses the 1-of-N encoding system. Based on the grasp type ([1-10]), the corresponding entry is set to 1, while the rest remain 0. The grasp parameters are converted to the coordinate system of the camera which was used to obtain the corresponding depth image. The transformed parameters are processed using a Fully-Connected (FC) layer consisting of 1024 nodes, and the output is \textit{element-wise added} to the visual features. The joint visual features and grasp parameter data are processed together in higher layers.

All FC layers have RELU activation functions, except for the output layer, which has softmax activations. The output layer has two nodes, corresponding to the success and failure probabilities of the grasp. A cross-entropy loss is used to train the neural network, as given in \eq\ref{equation:crossentropy}.

\begin{equation}
H_{y'}(y) := - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log (1-y_i)})
\label{equation:crossentropy}
\end{equation}
where $y_i'$ is the class label of the grasp, which is either 1 (success) or 0 (failure), and $y_i = f(I_i, h_i)$ is is the predicted label of the grasp pair ($I_i$, $h_i)$.

The proposed evaluative models EM1 and EM2 share the common features explained above. The models are introduced below. Only their unique properties are highlighted.

\begin{figure*}[t]
\centering
% \begin{center}
\subfloat[Evaluative Model 1]{%
  \includegraphics[width=0.95\textwidth]{images/networkArchitecture.pdf}
}
% \end{center}

% \begin{center}
\subfloat[Evaluative Model 2]{%
    \includegraphics[width=0.85\textwidth]{images/ResNet.pdf}
}
% \end{center}

% \begin{center}
\subfloat[Evaluative Model 3]{%
    \includegraphics[width=0.75\textwidth]{images/ChaoNet.png}
}
% \end{center}

\caption{The three evaluative network architectures presented in this paper. Similarly to Levine et al. \cite{Levine1}, the two channels of information (visual data and grasp parameters) are processed in parallel and combined to reach the final decision in each model. RELU activations are used throughout the models, except for the final softmax layers. A final softmax layer has grasp success and and failure nodes, and learns to predict the success probability of a grasp. (a) VGG-16 based model in our previous paper \cite{icra}. The first 13 convolutional layers of VGG-16 are frozen. (b) ResNet50-based \cite{resnet} proposed network. The first four blocks are used for feature extraction, and the rest of the network is used to learn joint features. (c) Third proposed model based on the VGG-16 arcihtecture. The information pathways are joined via concatenation, not addition.}

\label{fig:networkArchitecture2}
\end{figure*}

\subsection{Evaluative Model 1 (EM1)}

Figure \ref{fig:networkArchitecture} demonstrates the architecture of the first proposed evaluative network. The colourised depth image is processed with the VGG-16 network \cite{Simonyan14c} to obtain the high-level image features. The VGG-16 network is initialised with the weights obtained from ImageNet training. Only the last three convolutional layers are trained, and the first 13 layers remain frozen. This decision was made in order to speed up training.

The grasp parameters and image features pass through fully-connected layers with 1024 hidden nodes (FC-1024) layers in order to obtain two feature vectors of length 1024. The features are combined using the element-wise addition operation, and are further processed using 4 FC-1024 layers. Similarly with \cite{Levine1}, the features are combined using addition and not concatenation. In this thesis, this decision was made based on the observation that addition yielded a marginally better performance in the experiments (not included in this thesis). Furthermore, concatenation and addition can be considered as interchangeable operations when combining different information pathways in deep networks \cite{dumoulin2018feature-wise}. The final FC-1024 layers form the associations between the visual features and hand parameters, and contain most of the parameters in the network. 

\subsection{Evaluative Model 2 (EM2)}

% \begin{figure}[!ht]
%   \includegraphics[width=\textwidth]{images/ResNet.pdf}
%   \caption[The ResNet-based evaluative deep neural network architecture.]{The ResNet-based evaluative deep neural network architecture (EM2). Spatial tiling is used to repeat the grasp parameters before they join the image processing pathway. This network requires fewer FC layers due to the earlier marriage of information channels.}
% \label{fig:ResNet}
% \end{figure}

The second evaluative model, termed EM2, uses the ResNet-50 network in order to obtain the high-level image features. Similarly with VGG-16, the ResNet-50 architecture is initialised with the weights that have been obtained during ImageNet training. One exception is the last convolution block conv\_5x, which is initialised randomly. In the EM2 architecture, ResNet-50 network is broken down into two parts: the first 4 convolutional blocks are used to extract the visual features. The final convolutional block, which has 9 convolutional layers, combines the image features and grasp parameters. Similarly with EM1, element-wise addition joins the two channels of information. Spatial tiling is used to convert the processed grasp parameters, a vector of size $1024$, to a matrix of size $14 \times 14 \times 1024$. Because the last convolutional block conv\_5x processes combined information, this network is designed with only 2 FC layers with 64 hidden nodes each. The output layer is the same as EM1. 

\subsection{Evaluative Model 3 (EM3)}

This model, similarly with EM1, uses VGG-16 network as the visual model. All 16 layers of VGG-16 are unfrozen. The hand trajectory parameters go through feature extraction network before being concatenated with the visual features for further processing. Two fully connected layers of higher capacity than the previous evaluative models, containing 4096 hidden nodes each, joins the two information processing pathways. A final 2-node FC layer with softmax activation is added to obtain the final decision.

TODO: More info will be added here after getting various details from Chao, including custom feature extraction layer and output dimensionality for VGG-16.

All models were trained on simulated data and tested on the real robot, as well as in simulation. The next section focuses on the collection of the training data.