\noindent
The grasping system proposed, shown in Figure \ref{fig:systemArchitecture}, consists of a learned generative model and an evaluative model. The generative model is a method that generates a number of candidate grasps given a point cloud, as explained in the previous section. An evaluative model is paired with a generative model in order to estimate a probability of success for each candidate grasp. All evaluative models process the visual data and hand trajectory parameters in separate pathways, and combine them to feed into a third processing block to produce the final success probability. In addition, we present techniques for grasp optimisation using the EM as the objective function, using both Gradient Ascent (GA) and Simulated Annealing (SA). Finally, we train each evaluative model with data sets of simulated grasps generated using the generative models GM1 and GM2. Table \ref{table:GEBreakdown} shows a the full list of 17 variants we test.

In this section, the three proposed evaluative model (EM) architectures are explained. The grasp generator models, GM1 and GM2, given in the previous section, require very little training data to train, here being trained from 10 example grasps. %GM1 can generate 500 candidate grasps, ranked according to their estimated likelihoods, within 10 seconds on a 2x Intel Xeon E5-2650 v2 Eight Core 2.6GHz. GM2 takes a 50 seconds to create 250 grasps in the same setting. 
These generative models do not, however, estimate a probability of success for the generated grasps. An evaluative model, which is a Deep Neural Network (DNN), is used specifically for this purpose. DNNs have shown good performance in learning to evaluate grasps using grippers \cite{levine16,lenz2015deep}. They have also been applied to generating pre-grasps, so as to perform power grasps with dexterous hands \cite{varley2015generating,lu2017planning}.

%The generative approach ignores the global information about the object, such as overall shape and the object category. The success of an executed grasp, however, depends on many contextual factors such as full object shape, mass, mass distribution and surface friction. An evaluative network can indirectly learn to predict grasp success from image data. 
%The data provided to the evaluative network is collected from randomly generated scenes, therefore each scene has a different random combination of the parameters. The primary purpose of the network is to learn robust grasps across different conditions, and this is a complex task. The first challenge is that the kinematic model of the hand is unknown to the evaluative network. It only has access to the parameters that \textit{configure} the hand: the wrist and joint positions. Second, the system is weakly supervised with the grasp result (success or failure), and no further labels are provided.

We tested three evaluative models. The first is based on the VGG-16 network \cite{Simonyan14c}, named Evaluative Model 1 (EM1), and shown in Figure \ref{fig:networkArchitecture2} (a). A version based on the ResNet-50 network, termed EM2, is shown in Figure \ref{fig:networkArchitecture2} (b). Finally, EM3 (Figure \ref{fig:networkArchitecture2} (c)) is also based on VGG-16. All EMs are initialised with ImageNet weights. Regardless of the type, an EM has the functional form $f(I_t, h_t)$, where $I_t$ is a colourised depth image of the object,\footnote{Colourisation is a process by which additional channels are added to create a 3 channel representation. Here we add a channel encoding mean curvature and one encoding Gaussian curvature at that point in the depth image. We base this on \cite{kopicki2015ijrr}. This computation takes 0.01s on an Apple M1 processor (8 cores) for a depth image of size $224 \times 224$.} and $h_t$ contains a series of wrist poses and joint configurations for the hand, converted to the camera's frame of reference.The network's output layer calculates a probability of success for the image-grasp pair $I_t$, $h_t$. The model processes the grasp parameters and colourised depth information in separate channels, and combines them to feed into a feedforward pipeline that produces the output.
\begin{table}[b]
\footnotesize
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Variant & GM/  & EM & Opt'  & Training Set \\ 
 & Testset & & Meth' & \\ \hline
V1 & GM1    & - & - & 10 grasps  \\ \hline
V2 & GM2    & - & - & 10 grasps  \\ \hline
V3 & GM1/DS1-Te & EM1 & - & DS1-Tr \\ \hline
V4 & GM1/DS1-Te & EM2 & - & DS1-Tr \\ \hline
V5 & GM1/DS1-Te & EM3 & - & DS1-Tr  \\ \hline
V6 & GM1/DS1-Te & EM1 & - & DS1-Tr + DS2-Tr \\ \hline
V7 & GM1/DS1-Te & EM2 & - & DS1-Tr + DS2-Tr \\ \hline
V8 & GM1/DS1-Te & EM3 & - & DS1-Tr + DS2-Tr \\ \hline
V9 & GM2/DS2-Te & EM1 & - & DS1-Tr + DS2-Tr \\ \hline
V10 & GM2/DS2-Te & EM2 & - & DS1-Tr + DS2-Tr \\ \hline
V11 & GM2/DS2-Te & EM3 & - & DS1-Tr + DS2-Tr \\ \hline
V12 & GM1/DS1-Te & EM3 & GA1 & DS1-Tr + DS2-Tr \\ \hline
V13 & GM1/DS1-Te & EM3 & GA2 & DS1-Tr + DS2-Tr \\ \hline
V14 & GM1/DS1-Te & EM3 & GA3 & DS1-Tr + DS2-Tr \\ \hline
V15 & GM1/DS1-Te & EM3 & SA1 & DS1-Tr + DS2-Tr \\ \hline
V16 & GM1/DS1-Te & EM3 & SA2 & DS1-Tr + DS2-Tr \\ \hline
V17 & GM1/DS1-Te & EM3 & SA3 & DS1-Tr + DS2-Tr \\ \hline
\end{tabular}
\caption{The evaluated combinations of architecture, generative model/test set, training set, and optimisation method (Gradient Ascent (GA) or Stochastic Simulated Annealing (SA).}
\label{table:GEBreakdown}
\end{table}

%\begin{figure}[h]
%  \includegraphics[width=0.9\linewidth]{images/colourDepth.pdf}
%  \caption[Colourised depth images.]{Colourised depth images. From left to right, the objects are: coke bottle, chocolate box, hand cream, and bowl.}
%\label{fig:colorisedDepth}
%\end{figure}
The single-channel depth image is colourised before it is passed as input to the evaluative network. For this, we first crop the middle $460 \times 460$ section of the $640 \times 480$ depth image, and down-sample it to $224 \times 224$. Two more channels of the same dimensions are added, corresponding to the mean curvature and the Gaussian curvature. %Figure~\ref{fig:colorisedDepth} contains four examples of colourised depth images. 
This procedure provides meaningful depth features to the network, and makes the input compatible with VGG-16 and ResNet-50, which require input images of size $224 \times 224 \times 3$.

The grasp parameter data $h_t$ consists of 10 trajectory waypoints represented by $27 \times 10 = 270$ floating point numbers, and 10 extra numbers reserved for the grasp type.\footnote{Hand trajectory parameters are represented in the camera frame. Each waypoint has 27 float values: wrist position $(x,y,z)$ , wrist orientation (quaternion); finger joint angles (4 joints per finger, 5 fingers). Overall: 280 (10 + 10 * 27) floats per grasp.} Using trajectory information gives the network information it can use to predict unanticipated collisions with the object that will cause the grasp to fail. Each of the 10 training grasps is treated as a different class, and $h_t$ uses the 1-of-N encoding system. The grasp parameters are converted to the coordinate system of the camera which was used to obtain the corresponding depth image. In EM1 and EM2, the parameters are processed with a fully-connected (FC-1024) layer and \textit{element-wise added} to the learned visual representations, while EM3 uses a convolutional approach. All networks join the learned visual features and grasp parameters in higher layers.

All FC layers have RELU activation functions, except for the output, which uses 2-way softmax in all EM variants. The output layer has two nodes, corresponding to the grasp success and failure probabilities. Cross-entropy loss is used. % to train the neural network, as given in \eq\ref{equation:crossentropy}.

All evaluative models in Table \ref{table:GEBreakdown} were trained and tested on simulated data. EM2 and EM3 were tested on the real robot setup. Variants V3-V5 were trained using DS1-Tr. Variants V6-V17 were trained using the combined data set from DS1-Tr and DS2-Tr \footnote{10\% of DS1-Tr failure cases were sampled from the grasps that collide with the table, and we preserved the colliding grasps in DS1-V. This was to ensure EMs do not propose such grasps. Training with DS2 did not involve colliding grasps since overall grasp quality of GM2 is better.}. The Gradient Descent(GD) optimiser was employed with starting learning rate of 0.01, a dropout rate of 0.5, and early stopping. We halve the learning rate every 5 epochs during training.


%\begin{equation}
%H_{y'}(y) := - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log (1-y_i)})
%\label{equation:crossentropy}
%\end{equation}
%where $y_i'$ is the class label of the grasp, which is either 1 (success) or 0 (failure), and $y_i = f(I_i, h_i)$ is is the predicted label of the grasp pair ($I_i$, $h_i)$.
The models are introduced below. Only their unique properties are highlighted.

\begin{figure*}[t!]
\centering
% \begin{center}
\subfloat[Evaluative Model 1]{%
  \includegraphics[width=0.91\textwidth]{images/networkArchitecture.pdf}
}
% \end{center}

% \begin{center}
\subfloat[Evaluative Model 2]{%
    \includegraphics[width=0.91\textwidth]{images/ResNet.pdf}
}
% \end{center}

% \begin{center}
\subfloat[Evaluative Model 3]{%
    \includegraphics[width=0.91\textwidth]{images/Chaonet_newpic.pdf}
}
% \end{center}
\caption{The three evaluative network architectures. (a) EM1, a VGG-16 based model, where the first 13 layers of VGG-16 are frozen. (b) EM2, a ResNet-50-based network. The first four blocks are used for feature extraction and the rest to learn joint features. (c) EM3, also based on VGG-16.}
\label{fig:networkArchitecture2}
\end{figure*}

\subsection{Evaluative Models}

\noindent {\bf Evaluative Model 1 (EM1).}
\noindent
Figure~\ref{fig:networkArchitecture2} (a) shows the architecture of the first evaluative network. The colourised depth image is processed with the VGG-16 network \cite{Simonyan14c}. The first 13 layers are frozen to reduce overfitting. Grasp parameters and image features each pass through two FC-1024 layers to obtain two feature vectors of length 1024. The features are combined using element-wise addition and fed into 4 FC-1024 layers. Concatenation and addition can be considered as interchangeable in this context \cite{dumoulin2018feature-wise}. The final FC-1024 layers form associations between visual features and hand parameters, and contain most of the trainable parameters in the network.

\noindent {\bf Evaluative Model 2 (EM2).}
\noindent
% \begin{figure}[!ht]
%   \includegraphics[width=\textwidth]{images/ResNet.pdf}
%   \caption[The ResNet-based evaluative deep neural network architecture.]{The ResNet-based evaluative deep neural network architecture (EM2). Spatial tiling is used to repeat the grasp parameters before they join the image processing pathway. This network requires fewer FC layers due to the earlier marriage of information channels.}
% \label{fig:ResNet}
% \end{figure}
EM2 (Figure~\ref{fig:networkArchitecture2} (b)) uses the ResNet-50 architecture \cite{HeZRS15}, broken down into two parts. The first 4 convolutional blocks are used to extract the visual features. The final block, with 9 randomly-initialised convolutional layers, combines these with the grasp parameters. Similar to EM1, element-wise addition joins the two channels. The grasp parameters, a vector of size $1024$, are repeated to form a block of size $14 \times 14 \times 1024$. The final block processes the merged information and has 2 FC-64 layers.

\noindent {\bf Evaluative Model 3 (EM3).}
\noindent
This model (Figure~\ref{fig:networkArchitecture2} (c)) also uses VGG-16, but all 16 layers are trainable. The hand trajectory parameters pass through a feature extraction network before being concatenated with the visual features. The combined part of the network contains two high-capacity FC-4096 layers, followed by a FC2+softmax layer. EM3, in contrast to EM1 and EM2, uses convolutional layers for processing input grasp trajectories. The trajectory sub-network is similar to VGG-16 in that it contains 5 blocks, comprising 13 convolutional layers. The convolutional filters have a width of 3. The sizes under the blocks are input dimensions. Global Average Pooling (GAP) is used to obtain 512 features from each side, which are concatenated and passed through two FC-4096 layers. 

\subsection{Grasp optimisation using the EM}
\noindent
So far we have considered only generative-evaluative architectures where the evaluative model (EM) merely ranks the grasp proposals. As proposed by Lu et al. \cite{lu2017planning} the EM can be used to improve grasp proposals. This boils down to searching the grasp space driven by the EM as the objective function. This may be by gradient ascent or simulated annealing. The methods V12-17 use V8 as the objective function, hence V8 should be treated as the baseline. We employed both gradient based optimisation and simulated annealing.

\subsubsection{Gradient based optimisation}
\noindent
Lu et al. \cite{lu2017planning} proposed gradient ascent (GA), modifying the grasp parameters input to the EM with respect to the output, predicted success probability. They initialised with a heuristically selected pre-grasp. It is an interesting question whether we can apply their method to our evaluative models so as to further improve the performance. We therefore apply their technique initialised with the highest ranked grasp according to the EM. We investigated three variants:
\begin{itemize}
\item GA1: Shifts the position of all of the waypoints in the grasp trajectory equally. The gradient is the average position gradient across all 10 waypoints.
\item GA2: Tunes the hand configuration by tuning the angle of each finger joint. Every finger joint at each waypoint is treated independently.
\item GA3: Performs GA1 and GA2 simultaneously.
\end{itemize}

\subsubsection{Simulated annealing based optimisation}
\noindent
Gradient based optimisation is sensitive to the quality of gradient estimates derived from the model. Simulated annealing (SA) based optimisation is more robust to such noise. Therefore, three optimisation routines were implemented using SA:
\begin{itemize}
\item SA1: Shifts the positions of the all waypoints in the grasp trajectory equally. Moves are drawn from a three-dimensional Gaussian with $\mu=0$ and $\sigma=0.001$. 
\item SA2: Scales the angles of the finger joints in the final grasp pose with a single scaling parameter drawn from a Gaussian with $\mu=1$ and $\sigma=0.001$. The initial finger joint angles remain fixed and joint angles of the intermediate waypoints are linearly interpolated. 
\item SA3: Performs SA1 and SA2 simultaneously.
\end{itemize}
