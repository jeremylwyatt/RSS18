A variety of approaches have been taken to dexterous grasping \cite{bicchi2000a,miller2004,ciocarlie2009hand,detry2010a,varley2015generating,kopicki2015ijrr,Hua2016,Alo2016,Kanoulas2017,Cao2021}. There are four broad approaches to grasp planning for dexterous hands. First, there are methods that employ analytic mechanics to evaluate grasp quality. Second, there is work that engineers a mapping from sensing to grasp. Third, this mapping can be learned, such as learning a probabilistic generative model \cite{bishop_pattern_2006}. Fourth, it is also possible to learn a mapping from sensing and a grasp to a grasp success prediction. See \cite{bohg2014data} and  \cite{sahbani2012overview} for recent reviews of data-driven and analytic methods respectively.

Analytic approaches use mechanical models to predict grasp outcome \cite{bicchi2000a,Liu2000,Pollard2004,miller2004}. This requires models of both object (mass, mass distribution, shape, and surface friction) and manipulator (kinematics, exertable forces and torques). Several grasp quality metrics can be defined using these~\cite{Ferrari1992,Roa2015,Shimoga1996} under a variety of mechanical assumptions. These have been applied to dexterous grasp planning \cite{Boutselis2014,Gori2014,Hang2014,Rosales2012,Saut2012,ciocarlie2009hand}. The main drawback of analytic approaches is that estimation of object properties is hard. Even a small error in estimated shape, friction or mass will render a grasp unstable \cite{zheng2005a}. There is also evidence that grasp quality metrics are not well correlated with actual grasp success \cite{bekiroglu2011b,kim2013a,goins2014a}. It has been shown that a sampling based grasp planner, using an analytic evaluation model, can achieve grasp success of 53\% on real objects with some parameterized shape uncertainty \cite{li2016dexterous}. There are also hybrid methods \cite{Deng2021} where a neural network is used to generate grasp types and this then constrains more conventional search methods and evaluation functions based on grasp stability, reachability and heuristics governing the relationship of the finger contacts to the local surface geometry.

An alternative is learning for robot grasping, which has made steady progress. There are probabilistic machine learning techniques employed for surface estimation for grasping \cite{dragiev2011gaussian}; data efficient methods for learning grasps from demonstration \cite{ben-amor2012a,kopicki2015ijrr,Osa2018}; logistic regression for classifying grasp features from images \cite{saxena2008a}; extraction of generalisable parts for grasping \cite{detry2012a} and for autonomous grasp learning \cite{detry2010a}. Deep learning is a recent approach. Most work is for two-finger grippers e.g. \cite{songiros20}. Approaches either learn an evaluation function for an image-grasp pair \cite{levine16,lenz2015deep,gualtieri2016high,mahler2017dex,pinto2016supersizing,johns2016deep}, learn to predict the grasp parameters \cite{redmon2015real,kumra2017iros} or jointly estimate both \cite{morrison18}. The quantity of real training grasps can be reduced by mixing real and simulated data \cite{bousmalis2017using}. 


\begin{table*}[t]
\centering
\small
\resizebox{\columnwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
References & \multicolumn{3}{|c|}{Grasp type} & Robot & Clutter & Model & Novel  \\ \cline{2-4}
                   & 2-fing. & $>$2-fing. & $>$2-fing. & results & & free & objects\\ 
                   &            & pow.           & dext. &           &  &        &           \\ \hline
\cite{detry2012a,saxena2008a,detry2010a,lenz2015deep,mahler2017dex,johns2016deep,morrison-RSS-18} & \Checkmark & & & \Checkmark &  & \Checkmark & \Checkmark \\ \hline      
\cite{pinto2016supersizing,bousmalis2017using,levine2017,Gualtieri2016,songiros20} & \Checkmark & & & \Checkmark & \Checkmark & \Checkmark & \Checkmark \\ \hline     
\cite{kappler2015leveraging} &  & \Checkmark & &  &  &  & \Checkmark \\ \hline
\cite{zhou20176dof} &  & \Checkmark & &  &  & \Checkmark & \Checkmark \\ \hline     
\cite{lu2017planning,varley2015generating,detry2013a} &  & \Checkmark & & \Checkmark &  & \Checkmark & \Checkmark \\ \hline    
\cite{ben-amor2012a} &  & &  \Checkmark &  \Checkmark &  &  & \Checkmark \\ \hline    
\cite{veres2017modeling,zhou20176dof,kappler2015leveraging,mandikal2021dexterous,Shao2020} &  & &  \Checkmark &   &  & \Checkmark & \Checkmark \\ \hline 
\cite{kopicki2015ijrr,kopicki2019ijrr,Osa2018,Shang2020,Zhao2020,Liang22} &  & &  \Checkmark &  \Checkmark &  & \Checkmark & \Checkmark \\ \hline 
\cite{arruda2016active,Corsaro2021} &  & &  \Checkmark &  \Checkmark & \Checkmark & \Checkmark & \Checkmark \\ \hline 
This paper &  & &  \Checkmark &  \Checkmark &  & \Checkmark & \Checkmark \\ \hline 
\end{tabular}}
\caption{Qualitative comparison of grasp learning methods.}
\label{tab:comp-related-work}
\end{table*}

\begin{table}[t]
\small
\begin{center}
\caption{Comparison of reported real-robot results for dexterous grasps of novel objects from a single view. \label{tab:robot-results-comparison}}
\begin{tabular}{|c|c|c|c|c|} \hline
Paper & \# test   & total         & per alg     & \% succs \\ \
           & objects & \# grasps & \# grasps &              \\ \hline
Varley 15 \cite{varley2015generating} & 5  & 8 & 8  & 75 \%\\
Lu  17 \cite{lu2017planning} & 5 & 25  & 25 & 88  \% \\
Kopicki 16 \cite{kopicki2015ijrr} & 43 & 90  & 45 & 77.8\% \\
Kopicki 19 \cite{kopicki2019ijrr} & 40 & 589  & 49 & 81.6\% and 87.8\% \\
Arruda 16 \cite{arruda2016active} & 14 & 112 & 56 & 80.4\% \\
Detry 13 \cite{detry2013a} & 9 & 55 & 55 & 83.6\% \\
Osa 18  \cite{Osa2018} & 10  & 50 & 50 & 90.0\%\\
Shao 20 \cite{Shao2020} & 22 & 369 & 369 & 90.6\% \\
Shang 20 \cite{Shang2020} & 10 & 300 & 300 & 92.0\% \\
Corsaro 21 \cite{Corsaro2021} & 29 & 125 & 125 & 80.8\% \\
Zhao 20 \cite{Zhao2020} & 6 & 60 & 60 & 95\% \\
Liang 22 \cite{Liang22} & 14 & 560 & 140 & 82.9\% \\
this paper  & 40 & 196 & 49 & 87.8\% \\
\hline
\end{tabular}
\end{center}
\end{table}
Some papers have explored deep learning as a method for dexterous grasping. \cite{lu2017planning,varley2015generating,veres2017modeling,zhou20176dof,kappler2015leveraging,Zhu2021,Corsaro2021}. Several of these use simulation to generate the training set for learning. One approach \cite{Corsaro2021} is to generate a grasp using heuristics and to then assess that grasp with an evaluative model learned from simulated grasps. Kappler \cite{kappler2015leveraging} showed the ability of a CNN to predict grasp quality for multi-fingered grasps, but used complete point clouds as object models and only varied the wrist pose for the pre-grasp position.\footnote{Zhu \cite{Zhu2021} also used complete point clouds and information from a semantic segmentation to produce dexterous grasps that also functional, i.e. they are appropriate to the functional use of the grasped object. While neither of these results were extended to partial point clouds, other papers, such as \cite{kopicki2015ijrr} show that learning methods trained on full point clouds decline in performance when presented with partial points clouds.} Varley \cite{varley2015generating} and later Zhou \cite{zhou20176dof} went further by varying the hand pre-shape, and predicting from a single image of the scene. Each posed search for the grasp as a pure optimisation problem (using simulated annealing or quasi-Newton methods) on the output of the CNN. They, also, learned an evaluative model, and generated candidates for evaluation uninfluenced by prior knowledge. Veres \cite{veres2017modeling}, in contrast, learned a deep generative model. Lu \cite{lu2017planning} learned an evaluative model, and then, given an input image, optimised the inputs that describe the wrist pose and hand pre-shape to this model via gradient ascent, but did not learn a generative model. In addition, the grasps started with a heuristic grasp which was varied within a limited envelope. A restriction of both of these methods is that they only plan the pre-grasp, not the finger-surface contacts, and are thus limited to power-grasps. Shao \cite{Shao2020} presented a method that in a single generative-evaluative model both predicts sets of grasp contact points and ranks them by success probability. Mandikal \cite{mandikal2021dexterous} used reinforcement learning in simulation to learn a wide range of dexterous grasp policies that transfer to unseen objects and which do not require a full mesh. The results are simulated and the success rate for unseen objects is 53.1\%.\footnote{Personal correspondence with the authors.} Osa et. al \cite{Osa2018} also employed reinforcement learning in simulation and showed transfer of the learned grasping policies to real objects. Of the papers on dexterous grasp learning for novel objects, a subset have been tested on real grasps (Table~\ref{tab:robot-results-comparison}). 

Table~\ref{tab:comp-related-work} compares the properties of the learning methods reviewed above against this paper. Most works concern pinch grasping. Of the papers on learning methods for dexterous grasping, three \cite{varley2015generating,lu2017planning,detry2013a} are limited to power grasps, and five have no real robot results\cite{veres2017modeling,zhou20176dof,kappler2015leveraging,mandikal2021dexterous,Zhu2021}. Of the remaining six, two we directly build on here. The most comparable methods are \cite{Shao2020,Shang2020,Zhao2020,Liang22}. Shao \cite{Shao2020} employs both a generative and an evaluative model, although it only permits planning of 2 or 3 contact points with the object. Zhao \cite{Zhao2020} employs a CNN to generate a grasp, followed by an evaluative model that predicts grasp quality based on simulated data. Shang \cite{Shang2020} employs a learned generative model in the form of five sequenced convolutional neural networks (CNNs) that output the grasp configuration of a dexterous hand given the input image and an initial dexterous hand pose. Liang \cite{Liang22} uses a subspace learner for hand-shapes based on PCA and then refines parameters controlling the grasp and its execution using reinforcement learning in simulation. The difference of our work relative to these is that it combines: (i) a method able to  generate a wider range of dexterous grasps than \cite{varley2015generating,lu2017planning,detry2013a,Shao2020,Shang2020,Zhao2020} involving 2-11 finger links with (ii) either a higher success rate or a larger number of test objects than previous models that show this grasp flexibility. \cite{Liang22} We compare the evaluations performed by each paper in Table~\ref{tab:robot-results-comparison}) . 


%n learning to grasp divides into two categories, that we label {\em generative} and {\em evaluative} respectively. We quickly summarise the relationship between this paper and these two.
%
%Generative approaches to grasping include the work of detry et al, kopicki et al, these methods learn distributions of grasps from positive examples. In the case of detry, saxena, these are pinch grasps that are associated with features extracted from monocular or stereo images. Both those pieces of work were restricted to pinch grasps. Both Saxena and Detry require significant training samples, but Detry used real grasps, whereas Saxena used simulation to generate training exemplars. Other approaches use LfD rather than autonomous exploration, as this can significantly reduce the training data required. Peters et al introduced a method for generative grasping by grasp warping, demonstrating an ability to handle new objects of a similar global shape to the training objects. Kopicki et al introduced a factored generative model, showing grasp transfer to novel objects from one example of each grasp type. This is the method than we replicate and build on in our work.
%
%Evaluative approaches to grasping include the work of levine, in which a farm of fourteen robot were used over a two month data gathering period. This data is used to train a neural network that predicts the probability of success of a pinch grasp conditional on an image. In that work the only input is an RGB image, but other approaches to deep grasping tenpas use depth images. However, all these approaches are all data intensive. 
%
%The work that is closest to ours, and the only paper of which we are aware on deep learning applied to dexterous grasping, is that of Hermans et al. 