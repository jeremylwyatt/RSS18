There are four broad approaches to grasp planning for dexterous hands. First, we may employ analytic mechanics to evaluate grasp quality. Second, we may engineer a mapping from sensing to grasp. Third, we may learn this mapping, such as learning a generative model. Fourth, we may learn a mapping from sensing and a grasp to a grasp success prediction. See \cite{bohg2014data} and  \cite{sahbani2012overview} for recent reviews of data driven and analytic methods respectively.

Analytic approaches use mechanical models to predict grasp outcome \cite{bicchi2000a,Liu2000,Pollard2004,miller2004}. This requires models of both object (mass, mass distribution, shape, and surface friction) and manipulator (kinematics, exertable forces and torques). Several grasp quality metrics can be defined using these~\cite{Ferrari1992,Roa2015,Shimoga1996} under a variety of mechanical assumptions. These have been applied to dexterous grasp planning \cite{Boutselis2014,Gori2014,Hang2014,Rosales2012,Saut2012,ciocarlie2009hand}. The main drawback of analytic approaches is that estimation of object properties is hard. Even a small error in estimated shape, friction or mass will render a grasp unstable \cite{zheng2005a}. There is also evidence that grasp quality metrics are not well correlated with actual grasp success \cite{bekiroglu2011b,kim2013a,goins2014a}. It has been shown that a sampling based grasp planner, using an analytic evaluation model, can achieve grasp success of 53\% on real objects with some parameterized shape uncertainty \cite{li2016dexterous}.

An alternative is learning for robot grasping, which has made steady progress. There are probabilistic machine learning techniques employed for surface estimation for grasping \cite{dragiev2011gaussian}; data efficient methods for learning dexterous grasps from demonstration \cite{ben-amor2012a,kopicki2015ijrr,Osa2018}; logistic regression for classifying grasp features from images \cite{saxena2008a}; extracting generalisable parts for grasping \cite{detry2012a} and for autonomous grasp learning \cite{detry2010a}. Deep learning is a recent approach to grasping. Most work is for two finger grippers e.g. \cite{songiros20}. Approaches either learn an evaluation function for an image-grasp pair \cite{levine16,lenz2015deep,gualtieri2016high,mahler2017dex,pinto2016supersizing,johns2016deep}, learn to predict the grasp parameters \cite{redmon2015real,kumra2017iros} or jointly estimate both \cite{morrison18}. The quantity of real training grasps can be reduced by mixing real and simulated data \cite{bousmalis2017using}. 


\begin{table*}[t]
\centering
\small
\resizebox{\columnwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
References & \multicolumn{3}{|c|}{Grasp type} & Robot & Clutter & Model & Novel  \\ \cline{2-4}
                   & 2-fing. & $>$2-fing. & $>$2-fing. & results & & free & objects\\ 
                   &            & pow.           & dext. &           &  &        &           \\ \hline
\cite{detry2012a,saxena2008a,detry2010a,lenz2015deep,mahler2017dex,johns2016deep,morrison-RSS-18} & \Checkmark & & & \Checkmark &  & \Checkmark & \Checkmark \\ \hline      
\cite{pinto2016supersizing,bousmalis2017using,levine2017,Gualtieri2016,songiros20} & \Checkmark & & & \Checkmark & \Checkmark & \Checkmark & \Checkmark \\ \hline     
\cite{kappler2015leveraging} &  & \Checkmark & &  &  &  & \Checkmark \\ \hline
\cite{zhou20176dof} &  & \Checkmark & &  &  & \Checkmark & \Checkmark \\ \hline     
\cite{lu2017planning,varley2015generating} &  & \Checkmark & & \Checkmark &  & \Checkmark & \Checkmark \\ \hline    
\cite{ben-amor2012a} &  & &  \Checkmark &  \Checkmark &  &  & \Checkmark \\ \hline    
\cite{veres2017modeling,zhou20176dof,kappler2015leveraging,mandikal2021dexterous} &  & &  \Checkmark &   &  & \Checkmark & \Checkmark \\ \hline 
\cite{kopicki2015ijrr,kopicki2019ijrr,Osa2018} &  & &  \Checkmark &  \Checkmark &  & \Checkmark & \Checkmark \\ \hline 
\cite{arruda2016active} &  & &  \Checkmark &  \Checkmark & \Checkmark & \Checkmark & \Checkmark \\ \hline 
This paper &  & &  \Checkmark &  \Checkmark &  & \Checkmark & \Checkmark \\ \hline 
\end{tabular}}
\caption{Qualitative comparison of grasp learning methods.}
\label{tab:comp-related-work}
\end{table*}

A much smaller number of papers have explored deep learning as a method for dexterous grasping. \cite{lu2017planning,varley2015generating,veres2017modeling,zhou20176dof,kappler2015leveraging}. All of these use simulation to generate the training set for learning. Kappler \cite{kappler2015leveraging} showed the ability of a CNN to predict grasp quality for multi-fingered grasps, but uses complete point clouds as object models and only varies the wrist pose for the pre-grasp position, leaving the finger configurations the same. Varley \cite{varley2015generating} and later Zhou \cite{zhou20176dof} went beyond this by varying the hand pre-shape, and predicting from a single image of the scene. Each of these posed search for the grasp as a pure optimisation problem (using simulated annealing or quasi-Newton methods) on the output of the CNN. They, also, take the approach of learning an evaluative model, and generate candidates for evaluation uninfluenced by prior knowledge. Veres \cite{veres2017modeling}, in contrast, learns a deep generative model. Lu \cite{lu2017planning} learns an evaluative model, and then, given an input image, optimises the inputs that describe the wrist pose and hand pre-shape to this model via gradient ascent, but does not learn a generative model. In addition, the grasps start with a heuristic grasp which is varied within a limited envelope. Mandikal uses reinforcement learning in simulation to learn a wide range of dexterous grasp policies that transfer to unseen objects and which do not require a full mesh \cite{mandikal2021dexterous}. The results are only in simulation and the success rate for unseen objects is 53.1\%.\footnote{Personal correspondence with the authors.} Of the papers on dexterous grasp learning with deep networks only two approaches \cite{varley2015generating,lu2017planning} have been tested on real grasps, with eight and five test objects each, producing success rates of 75\% and 84\% respectively. An key restriction of both of these methods is that they only plan the pre-grasp, not the finger-surface contacts, and are thus limited to power-grasps.

Thus, in each case, either an evaluative model is learned but there is no learned prior over the grasp configuration able to be employed as a generative model; or a generative grasp model is learned, but there is no evaluative model learned to select the grasp. Our technical novelty is thus to bring together a data-efficient method of learning a good generative model with an evaluative model. As with others, we learn the evaluative model from simulation, but the generative model is learned from a small number of demonstrated grasps. Table~\ref{tab:comp-related-work} compares the properties of the learning methods reviewed above against this paper. Most works concern pinch grasping. Of the eight papers on learning methods for dexterous grasping, two \cite{varley2015generating,lu2017planning} are limited to power grasps. Of the remaining six, four have no real robot results \cite{veres2017modeling,zhou20176dof,kappler2015leveraging,mandikal2021dexterous}. Of the remaining four, two we directly build on here, the third being a extension of one of those grasp methods with active vision. Finally, our real robot evaluation is extensive in comparison with competitor works on dexterous grasping, comprising 196 real grasps of 40 different objects.

%n learning to grasp divides into two categories, that we label {\em generative} and {\em evaluative} respectively. We quickly summarise the relationship between this paper and these two.
%
%Generative approaches to grasping include the work of detry et al, kopicki et al, these methods learn distributions of grasps from positive examples. In the case of detry, saxena, these are pinch grasps that are associated with features extracted from monocular or stereo images. Both those pieces of work were restricted to pinch grasps. Both Saxena and Detry require significant training samples, but Detry used real grasps, whereas Saxena used simulation to generate training exemplars. Other approaches use LfD rather than autonomous exploration, as this can significantly reduce the training data required. Peters et al introduced a method for generative grasping by grasp warping, demonstrating an ability to handle new objects of a similar global shape to the training objects. Kopicki et al introduced a factored generative model, showing grasp transfer to novel objects from one example of each grasp type. This is the method than we replicate and build on in our work.
%
%Evaluative approaches to grasping include the work of levine, in which a farm of fourteen robot were used over a two month data gathering period. This data is used to train a neural network that predicts the probability of success of a pinch grasp conditional on an image. In that work the only input is an RGB image, but other approaches to deep grasping tenpas use depth images. However, all these approaches are all data intensive. 
%
%The work that is closest to ours, and the only paper of which we are aware on deep learning applied to dexterous grasping, is that of Hermans et al. 