There are four broad approaches to grasp planning for dexterous hands. First, \textcolor{red}{there are methods that} employ analytic mechanics to evaluate grasp quality. Second, \textcolor{red}{there is work that} engineers a mapping from sensing to grasp. Third, \textcolor{red}{this mapping can be learned}, such as learning a \textcolor{red}{probabilistic generative} model \cite{bishop_pattern_2006}. Fourth, \textcolor{red}{it is also possible to} learn a mapping from sensing and a grasp to a grasp success prediction. See \cite{bohg2014data} and  \cite{sahbani2012overview} for recent reviews of data-driven and analytic methods respectively.

Analytic approaches use mechanical models to predict grasp outcome \cite{bicchi2000a,Liu2000,Pollard2004,miller2004}. This requires models of both object (mass, mass distribution, shape, and surface friction) and manipulator (kinematics, exertable forces and torques). Several grasp quality metrics can be defined using these~\cite{Ferrari1992,Roa2015,Shimoga1996} under a variety of mechanical assumptions. These have been applied to dexterous grasp planning \cite{Boutselis2014,Gori2014,Hang2014,Rosales2012,Saut2012,ciocarlie2009hand}. The main drawback of analytic approaches is that estimation of object properties is hard. Even a small error in estimated shape, friction or mass will render a grasp unstable \cite{zheng2005a}. There is also evidence that grasp quality metrics are not well correlated with actual grasp success \cite{bekiroglu2011b,kim2013a,goins2014a}. It has been shown that a sampling based grasp planner, using an analytic evaluation model, can achieve grasp success of 53\% on real objects with some parameterized shape uncertainty \cite{li2016dexterous}.

An alternative is learning for robot grasping, which has made steady progress. There are probabilistic machine learning techniques employed for surface estimation for grasping \cite{dragiev2011gaussian}; data efficient methods for learning grasps from demonstration \cite{ben-amor2012a,kopicki2015ijrr,Osa2018}; logistic regression for classifying grasp features from images \cite{saxena2008a}; extraction of generalisable parts for grasping \cite{detry2012a} and for autonomous grasp learning \cite{detry2010a}. Deep learning is a recent approach. Most work is for two-finger grippers e.g. \cite{songiros20}. Approaches either learn an evaluation function for an image-grasp pair \cite{levine16,lenz2015deep,gualtieri2016high,mahler2017dex,pinto2016supersizing,johns2016deep}, learn to predict the grasp parameters \cite{redmon2015real,kumra2017iros} or jointly estimate both \cite{morrison18}. The quantity of real training grasps can be reduced by mixing real and simulated data \cite{bousmalis2017using}. 


\begin{table*}[t]
\centering
\small
\resizebox{\columnwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
References & \multicolumn{3}{|c|}{Grasp type} & Robot & Clutter & Model & Novel  \\ \cline{2-4}
                   & 2-fing. & $>$2-fing. & $>$2-fing. & results & & free & objects\\ 
                   &            & pow.           & dext. &           &  &        &           \\ \hline
\cite{detry2012a,saxena2008a,detry2010a,lenz2015deep,mahler2017dex,johns2016deep,morrison-RSS-18} & \Checkmark & & & \Checkmark &  & \Checkmark & \Checkmark \\ \hline      
\cite{pinto2016supersizing,bousmalis2017using,levine2017,Gualtieri2016,songiros20} & \Checkmark & & & \Checkmark & \Checkmark & \Checkmark & \Checkmark \\ \hline     
\cite{kappler2015leveraging} &  & \Checkmark & &  &  &  & \Checkmark \\ \hline
\cite{zhou20176dof} &  & \Checkmark & &  &  & \Checkmark & \Checkmark \\ \hline     
\cite{lu2017planning,varley2015generating,detry2013a} &  & \Checkmark & & \Checkmark &  & \Checkmark & \Checkmark \\ \hline    
\cite{ben-amor2012a} &  & &  \Checkmark &  \Checkmark &  &  & \Checkmark \\ \hline    
\cite{veres2017modeling,zhou20176dof,kappler2015leveraging,mandikal2021dexterous,Shao2020} &  & &  \Checkmark &   &  & \Checkmark & \Checkmark \\ \hline 
\cite{kopicki2015ijrr,kopicki2019ijrr,Osa2018} &  & &  \Checkmark &  \Checkmark &  & \Checkmark & \Checkmark \\ \hline 
\cite{arruda2016active} &  & &  \Checkmark &  \Checkmark & \Checkmark & \Checkmark & \Checkmark \\ \hline 
This paper &  & &  \Checkmark &  \Checkmark &  & \Checkmark & \Checkmark \\ \hline 
\end{tabular}}
\caption{Qualitative comparison of grasp learning methods.}
\label{tab:comp-related-work}
\end{table*}

\begin{table}[t]
\small
\begin{center}
\caption{Comparison of reported real-robot results for dexterous grasps of novel objects from a single view. \label{tab:robot-results-comparison}}
\begin{tabular}{|c|c|c|c|c|} \hline
Paper & \# test   & total         & per alg     & \% succs \\ \
           & objects & \# grasps & \# grasps &              \\ \hline
Varley 15 \cite{varley2015generating} & 5  & 8 & 8  & 75 \%\\
Lu  17 \cite{lu2017planning} & 5 & 25  & 25 & 88  \% \\
Kopicki 16 \cite{kopicki2015ijrr} & 43 & 90  & 45 & 77.8\% \\
Kopicki 19 \cite{kopicki2019ijrr} & 40 & 589  & 49 & 81.6\% and 87.8\% \\
Arruda 16 \cite{arruda2016active} & 14 & 112 & 56 & 80.4\% \\
Detry 13 \cite{detry2013a} & 9 & 55 & 55 & 83.6\% \\
Osa 18  \cite{Osa2018} & 10  & 50 & 50 & 90.0\%\\
Shao 20 \cite{Shao2020} & 22 & 369 & 369 & 90.6\% \\
this paper  & 40 & 196 & 49 & 87.8\% \\
\hline
\end{tabular}
\end{center}
\end{table}
Some papers have explored deep learning as a method for dexterous grasping. \cite{lu2017planning,varley2015generating,veres2017modeling,zhou20176dof,kappler2015leveraging}. These use simulation to generate the training set for learning. Kappler \cite{kappler2015leveraging} showed the ability of a CNN to predict grasp quality for multi-fingered grasps, but used complete point clouds as object models and only varied the wrist pose for the pre-grasp position.\footnote{\textcolor{red}{While Kappler's results were not extended, other papers, such as \cite{kopicki2015ijrr} show that learning methods trained on full point clouds decline in performance when presented with partial points clouds.}} Varley \cite{varley2015generating} and later Zhou \cite{zhou20176dof} went further by varying the hand pre-shape, and predicting from a single image of the scene. Each posed search for the grasp as a pure optimisation problem (using simulated annealing or quasi-Newton methods) on the output of the CNN. They, also, learned an evaluative model, and generate candidates for evaluation uninfluenced by prior knowledge. Veres \cite{veres2017modeling}, in contrast, learned a deep generative model. Lu \cite{lu2017planning} learned an evaluative model, and then, given an input image, optimised the inputs that describe the wrist pose and hand pre-shape to this model via gradient ascent, but did not learn a generative model. In addition, the grasps started with a heuristic grasp which was varied within a limited envelope. A restriction of both of these methods is that they only plan the pre-grasp, not the finger-surface contacts, and are thus limited to power-grasps. \textcolor{red}{Shao \cite{Shao2020} presented a method that in a single generative-evaluative model predicts both sets of grasp contact points and ranks them by success probability.} Mandikal \cite{mandikal2021dexterous} used reinforcement learning in simulation to learn a wide range of dexterous grasp policies that transfer to unseen objects and which do not require a full mesh. The results are simulated and the success rate for unseen objects is 53.1\%.\footnote{Personal correspondence with the authors.} Osa et. al \cite{Osa2018} also employed reinforcement learning in simulation and showed transfer of the learned grasping policies to real objects. Of the papers on dexterous grasp learning for novel objects, only a subset have been tested on real grasps (Table~\ref{tab:robot-results-comparison}). 

Table~\ref{tab:comp-related-work} compares the properties of the learning methods reviewed above against this paper. Most works concern pinch grasping. Of the papers on learning methods for dexterous grasping, three \cite{varley2015generating,lu2017planning,detry2013a} are limited to power grasps, and four have no real robot results\cite{veres2017modeling,zhou20176dof,kappler2015leveraging,mandikal2021dexterous}. Of the remainder, two we directly build on here, one is an extension of one of those grasp methods with active vision. \textcolor{red}{The most comparable method in terms of its assumptions is that of Shao \cite{Shao2020}, although this only permits planning of 2 or 3 contact points with the object. The difference of our approach is that it combines: (i) a wider range of dexterous grasps than\cite{varley2015generating,lu2017planning,detry2013a,Shao2020} involving planned contacts between the object and 2-11 finger links; with (ii) a higher success rate than purely generative models \cite{kopicki2015ijrr,kopicki2019ijrr} that also allow this grasp flexibility.} We compare the evaluations performed by each paper in Table~\ref{tab:robot-results-comparison}). 


%n learning to grasp divides into two categories, that we label {\em generative} and {\em evaluative} respectively. We quickly summarise the relationship between this paper and these two.
%
%Generative approaches to grasping include the work of detry et al, kopicki et al, these methods learn distributions of grasps from positive examples. In the case of detry, saxena, these are pinch grasps that are associated with features extracted from monocular or stereo images. Both those pieces of work were restricted to pinch grasps. Both Saxena and Detry require significant training samples, but Detry used real grasps, whereas Saxena used simulation to generate training exemplars. Other approaches use LfD rather than autonomous exploration, as this can significantly reduce the training data required. Peters et al introduced a method for generative grasping by grasp warping, demonstrating an ability to handle new objects of a similar global shape to the training objects. Kopicki et al introduced a factored generative model, showing grasp transfer to novel objects from one example of each grasp type. This is the method than we replicate and build on in our work.
%
%Evaluative approaches to grasping include the work of levine, in which a farm of fourteen robot were used over a two month data gathering period. This data is used to train a neural network that predicts the probability of success of a pinch grasp conditional on an image. In that work the only input is an RGB image, but other approaches to deep grasping tenpas use depth images. However, all these approaches are all data intensive. 
%
%The work that is closest to ours, and the only paper of which we are aware on deep learning applied to dexterous grasping, is that of Hermans et al. 