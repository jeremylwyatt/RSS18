There are four broad approaches to grasp planning. First, we may employ analytic mechanics to evaluate grasp quality. Second, we may engineer a mapping from sensing to grasp. Third, we may learn this mapping, such as learning a generative model. Fourth, we may learn a mapping from sensing and a grasp to an prediction of grasp success. See \cite{bohg2014data} and  \cite{sahbani2012overview} for recent reviews of data driven and analytic methods respectively.

Analytic approaches employ mechanical models to predict grasp outcome\cite{bicchi2000a,Liu2000,Pollard2004,miller2004}. This requires models of both object (object mass, mass distribution, shape, and surface friction) and manipuator (kinematics, exertable forces and torques). Several grasp quality metrics can be defined using these~\cite{Ferrari1992,Roa2015,Shimoga1996} under a variety of mechanical assumptions. These have been applied to dexterous grasp planning \cite{Boutselis2014,Gori2014,Hang2014,Rosales2012,Saut2012,ciocarlie2009hand}. The main drawback of analytic approaches is that estimation of object properties is hard. Even a small error in estimated shape, friction or mass will render a grasp unstable \cite{zheng2005a}. There is also evidence that grasp quality metrics are not well correlated with actual grasp success \cite{bekiroglu2011b,kim2013a,goins2014a}.

An alternative is learning for robot grasping, which has made steady progress. There are probabilistic machine learning techniques employed for surface estimation for grasping \cite{dragiev2011gaussian}; data efficient methods for learning dexterous grasps from demonstration \cite{ben-amor2012a,kopicki2015ijrr,detry2012a}; logistic regression for classifying grasp features from images \cite{saxena2008a}; and for autonomous learning \cite{detry2010a}. Deep learning is a recent approach to grasping. Most work is for two finger grippers. Approaches either learn an evaluation function for an image-grasp pair \cite{levine16,lenz2015deep,gualtieri2016high,mahler2017dex,pinto2016supersizing,johns2016deep}, learn to predict the grasp parameters \cite{redmon2015real,kumra2017iros} or jointly estimate both \cite{morrison18}. The quantity of real training grasps can be reduced by mixing real and simulated data \cite{bousmalis2017using}. 


\begin{table*}[t]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
References & \multicolumn{3}{|c|}{Grasp type} & Robot & Clutter & Model & Novel  \\ \cline{2-4}
                   & 2-fing. & $>$2-finger & $>$2-finger & results & & free & objects\\ 
                   &            & power           & dexterous &           &  &        &           \\ \hline
\cite{detry2012a,saxena2008a,detry2010a,lenz2015deep,mahler2017dex,johns2016deep,morrison-RSS-18} & \Checkmark & & & \Checkmark &  & \Checkmark & \Checkmark \\ \hline      
\cite{pinto2016supersizing,bousmalis2017using,levine2017,Gualtieri2016} & \Checkmark & & & \Checkmark & \Checkmark & \Checkmark & \Checkmark \\ \hline     
\cite{kappler2015leveraging} &  & \Checkmark & &  &  &  & \Checkmark \\ \hline
\cite{zhou20176dof} &  & \Checkmark & &  &  & \Checkmark & \Checkmark \\ \hline     
\cite{lu2017planning,varley2015generating} &  & \Checkmark & & \Checkmark &  & \Checkmark & \Checkmark \\ \hline    
\cite{ben-amor2012a} &  & &  \Checkmark &  \Checkmark &  &  & \Checkmark \\ \hline    
\cite{Boutselis2014,Gori2014,Hang2014,Rosales2012,ciocarlie2009hand,veres2017modeling,saut2012a} &  & &  \Checkmark &   &  & \Checkmark & \Checkmark \\ \hline \cite{kopicki2015ijrr,kopicki2019ijrr} &  & &  \Checkmark &  \Checkmark &  & \Checkmark & \Checkmark \\ \hline 
\cite{arruda2016active} &  & &  \Checkmark &  \Checkmark & \Checkmark & \Checkmark & \Checkmark \\ \hline 
This paper &  & &  \Checkmark &  \Checkmark &  & \Checkmark & \Checkmark \\ \hline 
\end{tabular}
\caption{Qualitative comparison of grasp learning methods.}
\label{tab:comp-related-work}
\end{table*}

A small number of papers have explored deep learning as a method for dexterous grasping. \cite{lu2017planning,varley2015generating,veres2017modeling,zhou20176dof,kappler2015leveraging}. All of these use simulation to generate the training set for learning. Kappler \cite{kappler2015leveraging} showed the ability of a CNN to predict grasp quality for multi-fingered grasps, but uses complete point clouds as object models and only varies the wrist pose for the pre-grasp position, leaving the finger configurations the same. Varley \cite{varley2015generating} and later Zhou \cite{zhou20176dof} went beyond this, each being able to vary the hand pre-shape, and predicting from a single image of the scene. Each of these posed search for the grasp as a pure optimisation problem (using simulated annealing or quasi-Newton methods) on the output of the CNN. They all, also, take the approach of learning an evaluative model, and generate candidates for evaluation uninfluenced by prior knowledge. Veres \cite{veres2017modeling}, in contrast, learns a deep generative model. Finally Lu \cite{lu2017planning} learns an evaluative model, and then, given an input image, optimises the inputs that describe the wrist pose and hand pre-shape to this model via gradient ascent, but does not learn a generative model. In addition, the grasps start with a heuristic grasp which is varied within a limited envelope. Of the papers on dexterous grasp learning with deep networks only two approaches \cite{varley2015generating,lu2017planning} have been tested on real grasps, with eight and five test objects each, producing success rates of 75\% and 84\% respectively. An important restriction of both of these methods is that they only plan the pre-grasp, not the finger-surface contacts and are thus limited to power-grasps.

Thus, in each case, either an evaluative model is learned but there is no learned prior over the grasp configuration able to employed as a generative model; or a generative grasp model is learned, but there is no evaluative model learned to select the grasp. Our novelty is thus to bring together a data-efficient method of learning a good generative model with an evaluative model. As with others, we learn the evaluative model from simulation, but the generative model is learned from a small number of demonstrated grasps. Table~\ref{tab:comp-related-work} compares the properties of the methods reviewed above against that in this paper. It can be seen that there are only five papers that consider fully dexterous grasping with a multi-finger hand--as opposed to using a multi-finger hand to perform a power grasp--which are object-model free. Of these, none 

%n learning to grasp divides into two categories, that we label {\em generative} and {\em evaluative} respectively. We quickly summarise the relationship between this paper and these two.
%
%Generative approaches to grasping include the work of detry et al, kopicki et al, these methods learn distributions of grasps from positive examples. In the case of detry, saxena, these are pinch grasps that are associated with features extracted from monocular or stereo images. Both those pieces of work were restricted to pinch grasps. Both Saxena and Detry require significant training samples, but Detry used real grasps, whereas Saxena used simulation to generate training exemplars. Other approaches use LfD rather than autonomous exploration, as this can significantly reduce the training data required. Peters et al introduced a method for generative grasping by grasp warping, demonstrating an ability to handle new objects of a similar global shape to the training objects. Kopicki et al introduced a factored generative model, showing grasp transfer to novel objects from one example of each grasp type. This is the method than we replicate and build on in our work.
%
%Evaluative approaches to grasping include the work of levine, in which a farm of fourteen robot were used over a two month data gathering period. This data is used to train a neural network that predicts the probability of success of a pinch grasp conditional on an image. In that work the only input is an RGB image, but other approaches to deep grasping tenpas use depth images. However, all these approaches are all data intensive. 
%
%The work that is closest to ours, and the only paper of which we are aware on deep learning applied to dexterous grasping, is that of Hermans et al. 